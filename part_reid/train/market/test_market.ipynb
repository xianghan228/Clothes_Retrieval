{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import caffe\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N=1;C=3;W=80;H=160\n",
    "crop_h=crop_w=0\n",
    "transformer = caffe.io.Transformer({'data': (N,C,H+2*crop_h,W+2*crop_w)})\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "transformer.set_mean('data', np.array([ 104,  117,  123])) # mean pixel\n",
    "transformer.set_raw_scale('data', 255)  # the reference model operates on images in [0,255] range instead of [0,1]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # the reference model has channels in BGR order instead of RGB\n",
    "\n",
    "\n",
    "def readImages(images):\n",
    "    imageLen=len(images)\n",
    "    imageDataList=[]\n",
    "    for imageIdx in range(imageLen):\n",
    "        imageName=images[imageIdx]\n",
    "        imageImage=transformer.preprocess('data', caffe.io.load_image(imageName))\n",
    "        imageDataList.append(imageImage[:,crop_h:H+crop_h,crop_w:W+crop_w]) #center crop\n",
    "        imageIdx+=1\n",
    "    #imageData and imageData\n",
    "    imageData=np.asarray(imageDataList)\n",
    "    return imageData\n",
    "\n",
    "def readDir(list_dir):\n",
    "    import os\n",
    "    file_list=os.listdir(list_dir)\n",
    "    final_list=[]\n",
    "    for filename in file_list:\n",
    "        if filename[0]!='-' and filename[filename.rfind('.')+1:]=='jpg':\n",
    "            final_list.append(list_dir+filename)\n",
    "    return final_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(file_list,net):\n",
    "    file_len=len(file_list)\n",
    "    features=[]\n",
    "    batch_size=100\n",
    "    for batch_idx in range(file_len/batch_size+1):\n",
    "        cur_len=batch_size if batch_idx <file_len/batch_size else file_len%batch_size\n",
    "        cur_list=file_list[batch_idx*batch_size+0:batch_idx*batch_size+cur_len]\n",
    "        image_data=readImages(cur_list)\n",
    "        net.blobs['data'].reshape(cur_len,C,H,W)\n",
    "        net.blobs['data'].data[:] = image_data\n",
    "        net.forward()\n",
    "        normed_features=net.blobs['normed_feature'].data.copy()\n",
    "        from sklearn.preprocessing import normalize\n",
    "        for idx in range(cur_len):\n",
    "            cur_feature=np.squeeze(normed_features[idx,:])\n",
    "#             cur_feature = cur_feature/np.linalg.norm(cur_feature)\n",
    "            features.append(cur_feature)\n",
    "    return features\n",
    "def get_gt_dict(gallery_list):\n",
    "    gt_dict={}\n",
    "    for idx in range(len(gallery_list)):\n",
    "        gallery_name=gallery_list[idx]\n",
    "        gallery_person_id=gallery_name[gallery_name.rfind('/')+1:gallery_name.rfind('/')+5]\n",
    "        gallery_cam_id=gallery_name[gallery_name.rfind('/')+7:gallery_name.rfind('/')+8]\n",
    "        if gt_dict.has_key(gallery_person_id):\n",
    "            gt_dict[gallery_person_id].append(idx)\n",
    "        else:\n",
    "            gt_dict[gallery_person_id]=[idx]\n",
    "    return gt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_for_queries(query_features,gallery_features):\n",
    "    import numpy as np\n",
    "    all_rank_list=[]\n",
    "    for query_idx in range(len(query_features)):\n",
    "        query_feature=query_features[query_idx]\n",
    "\n",
    "        score_list=[]\n",
    "        for gallery_idx in range(len(gallery_features)):\n",
    "            gallery_feature=gallery_features[gallery_idx]\n",
    "            dist = np.sqrt(np.sum((query_feature-gallery_feature)**2))\n",
    "            similar_score=1.0/(1.0+dist)\n",
    "            score_list.append(similar_score)\n",
    "        #we get scoreList, then cal predictLists\n",
    "        ranked_idx_list=np.argsort(score_list)[::-1]\n",
    "        all_rank_list.append(ranked_idx_list)\n",
    "    return all_rank_list\n",
    "\n",
    "######################################################\n",
    "##\n",
    "## I use parallel to run the query in batch_num=10 batches\n",
    "## In this way, one query on the 1w galleries takes 0.027s\n",
    "## Multi-process does not work for ipython notebook on Windows\n",
    "##\n",
    "######################################################\n",
    "def parallel_rank(query_features,gallery_features):\n",
    "    import ipyparallel as ipp\n",
    "    client = ipp.Client()\n",
    "    view = client.load_balanced_view()\n",
    "    batch_num=2*len(client.ids)\n",
    "    batch_size_queries=len(query_features)/batch_num+1\n",
    "\n",
    "    tic=time.time()\n",
    "    task_results=[]\n",
    "    for task_idx in range(batch_num):\n",
    "        batch_query_features=query_features[task_idx*batch_size_queries:(task_idx+1)*batch_size_queries]\n",
    "        task_results.append(view.apply(rank_for_queries,batch_query_features,gallery_features))    \n",
    "\n",
    "    all_rank_list=[]\n",
    "    for task_idx in range(batch_num):\n",
    "        all_rank_list.extend(task_results[task_idx].result())\n",
    "    toc=time.time()\n",
    "    print len(all_rank_list),(toc-tic),(toc-tic)/len(query_features)\n",
    "    return all_rank_list\n",
    "\n",
    "def evaluate(query_list,gallery_list,all_rank_list,gt_dict):\n",
    "    histogram=np.zeros(len(gallery_list))\n",
    "    meanAP=0.0\n",
    "    len_queries=len(query_list)\n",
    "    for query_idx in range(len_queries):#\n",
    "        ranked_idx_list=all_rank_list[query_idx]\n",
    "        #good or junk\n",
    "        query_name=query_list[query_idx]\n",
    "        query_person_id=query_name[query_name.rfind('/')+1:query_name.rfind('/')+5]\n",
    "        query_cam_id=query_name[query_name.rfind('/')+7:query_name.rfind('/')+8]\n",
    "        relevant_idx_list=gt_dict[query_person_id]\n",
    "        good_relevant=[]\n",
    "        junk_relevant=[]\n",
    "        for relevant_idx in relevant_idx_list:\n",
    "            gallery_name=gallery_list[relevant_idx]\n",
    "            gallery_cam_id=gallery_name[gallery_name.rfind('/')+7:gallery_name.rfind('/')+8]\n",
    "            if gallery_cam_id==query_cam_id:\n",
    "                junk_relevant.append(relevant_idx)\n",
    "            else:\n",
    "                good_relevant.append(relevant_idx)\n",
    "        #cmc and meanAP\n",
    "        matched_num=0.0\n",
    "        sum_precision=0.0\n",
    "        rank_idx=0\n",
    "        for perdicted_idx in ranked_idx_list:\n",
    "            if perdicted_idx in junk_relevant:\n",
    "                continue\n",
    "            elif perdicted_idx in good_relevant:\n",
    "                matched_num+=1.0\n",
    "                sum_precision+=matched_num/(rank_idx+1)\n",
    "                histogram[rank_idx]+= 1 if matched_num<=1 else 0 #multiple results\n",
    "            rank_idx+=1\n",
    "            if matched_num>=len(good_relevant): #recall=1\n",
    "                break\n",
    "        meanAP+=sum_precision/len(good_relevant)\n",
    "\n",
    "    cmc=np.cumsum(histogram)/len_queries\n",
    "    meanAP/=len_queries\n",
    "    return cmc*100.0,meanAP*100.0\n",
    "\n",
    "def save_log(model_name,cmc,meanAP):\n",
    "    log_name='log.txt'\n",
    "    log_file=open(log_name,'a')\n",
    "    log_str='\\n\\n'+'*'*65+'\\n'\n",
    "    log_str=log_str+ model_name+\",\"+\"%dx%d, mAP=%.2f\\n\"%(H,W,meanAP)\n",
    "    log_str=log_str+'{}\\n\\n'.format(cmc[:50])\n",
    "    log_str=log_str+'rank1\\trank5\\trank10\\trank20\\n'\n",
    "    log_str=log_str+'%.2f\\t%.2f\\t%.2f\\t%.2f\\n'%(cmc[0],cmc[4],cmc[9],cmc[19])\n",
    "    print log_str\n",
    "    log_file.write(log_str)\n",
    "    log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(EXP_DIR,PRETRAINED):\n",
    "    #init net\n",
    "    MODEL_FILE = EXP_DIR+'test.prototxt'\n",
    "    model_name=PRETRAINED[PRETRAINED.rfind('/')+1:-11]\n",
    "\n",
    "    caffe.set_device(1)\n",
    "    caffe.set_mode_gpu()\n",
    "    net = caffe.Classifier(MODEL_FILE, PRETRAINED,caffe.TEST)\n",
    "    \n",
    "    DATA_DIR= '../../dataset/market/Market-1501/'\n",
    "    query_list=readDir(DATA_DIR+'query/')\n",
    "    gallery_list=readDir(DATA_DIR+'bounding_box_test/')\n",
    "\n",
    "    gt_dict=get_gt_dict(gallery_list)\n",
    "    print len(query_list),len(gallery_list),len(gt_dict),len(gt_dict['0000'])\n",
    "    \n",
    "    ################extract features############\n",
    "    tic=time.time()\n",
    "    query_features=extract_features(query_list,net)\n",
    "    gallery_features=extract_features(gallery_list,net)\n",
    "    toc=time.time()\n",
    "    print (toc-tic),(toc-tic)/(len(query_list)+len(gallery_list))\n",
    "    \n",
    "    ################rank############\n",
    "    all_rank_list=parallel_rank(query_features,gallery_features)\n",
    "    cmc,mAP=evaluate(query_list,gallery_list,all_rank_list,gt_dict)\n",
    "    save_log('Market: '+EXP_DIR+model_name,cmc,mAP)\n",
    "    return cmc,mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3368 15913 751 2798\n",
      "57.3356001377 0.0029736839447\n",
      "3368 88.2150070667 0.026192104236\n",
      "\n",
      "\n",
      "*****************************************************************\n",
      "Market: ./partnet/model_iter_50000,160x80, mAP=63.42\n",
      "[ 80.90855107  86.31235154  88.92517815  90.82541568  91.74584323\n",
      "  92.42874109  93.14133017  93.76484561  94.35866983  94.68527316\n",
      "  94.804038    95.19002375  95.42755344  95.63539192  95.8432304\n",
      "  96.05106888  96.25890736  96.46674584  96.55581948  96.6152019\n",
      "  96.67458432  96.70427553  96.82304038  96.97149644  97.03087886\n",
      "  97.06057007  97.14964371  97.26840855  97.32779097  97.32779097\n",
      "  97.35748219  97.35748219  97.50593824  97.53562945  97.56532067\n",
      "  97.62470309  97.62470309  97.68408551  97.74346793  97.74346793\n",
      "  97.74346793  97.80285036  97.80285036  97.80285036  97.83254157\n",
      "  97.89192399  97.9216152   97.95130641  97.98099762  97.98099762]\n",
      "\n",
      "rank1\trank5\trank10\trank20\n",
      "80.91\t91.75\t94.69\t96.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXP_DIR='./partnet/'\n",
    "PRETRAINED = EXP_DIR+'snapshot/model_iter_50000.caffemodel'\n",
    "cmc,mAP=main(EXP_DIR,PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
